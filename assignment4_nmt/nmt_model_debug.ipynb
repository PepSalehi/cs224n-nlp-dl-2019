{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocab import *\n",
    "from model_embeddings import *\n",
    "from nmt_model import *\n",
    "from utils import *\n",
    "from sanity_check import *\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load some data that we may use to check our model. We use data from sanity check tests. \n",
    "\n",
    "As a reminder our `Vocab` class contains `src` and `tgt` vocabularies (of type `VocabEntry`) and methods to `build`, `save` and `load`. `VocabEntry` is a standard class that contains `word2id` dictionaries to encode our input (as a list of integers to feed into `Embedding` layer). In our testing case we have tiny vocabularies of under `100` entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab.load('./sanity_check_en_es_data/vocab_sanity_check.json') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 85)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.src), len(vocab.tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<pad>', 0),\n",
       " ('<s>', 1),\n",
       " ('</s>', 2),\n",
       " ('<unk>', 3),\n",
       " ('de', 4),\n",
       " ('que', 5),\n",
       " ('el', 6),\n",
       " ('en', 7),\n",
       " ('la', 8),\n",
       " ('a', 9)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.src.word2id.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<pad>', 0),\n",
       " ('<s>', 1),\n",
       " ('</s>', 2),\n",
       " ('<unk>', 3),\n",
       " ('the', 4),\n",
       " ('of', 5),\n",
       " ('to', 6),\n",
       " ('that', 7),\n",
       " ('and', 8),\n",
       " ('in', 9)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.tgt.word2id.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our `model` we need this `vocab` and a couple of arguments that are defined in `sanity_check.py`. We change `EMBED_SIZE` to distinguish it from `HIDDEN_SIZE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3, 3, 0.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE, EMBED_SIZE, HIDDEN_SIZE, DROPOUT_RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMT(\n",
    "    embed_size=EMBED_SIZE-1,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    "    vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NMT(\n",
       "  (model_embeddings): ModelEmbeddings(\n",
       "    (source): Embedding(77, 2, padding_idx=0)\n",
       "    (target): Embedding(85, 2, padding_idx=0)\n",
       "  )\n",
       "  (encoder): LSTM(2, 3, bidirectional=True)\n",
       "  (decoder): LSTMCell(5, 3)\n",
       "  (h_projection): Linear(in_features=6, out_features=3, bias=False)\n",
       "  (c_projection): Linear(in_features=6, out_features=3, bias=False)\n",
       "  (att_projection): Linear(in_features=6, out_features=3, bias=False)\n",
       "  (combined_output_projection): Linear(in_features=9, out_features=3, bias=False)\n",
       "  (target_vocab_projection): Linear(in_features=3, out_features=85, bias=False)\n",
       "  (dropout): Dropout(p=0.0)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load train data (both `source` and `target`). As we can see this is just a list of sentences in Spanish and English before encoding. \n",
    "\n",
    "Then we construct a batch of `BATCH_SIZE` and *sort* sentences by their length. This is necessary as we know for `pack_padded_sequence()` function.\n",
    "\n",
    "The last step is to encode these sentences into list of integers using our `word2id` dictionaries. Shape is changed to `(seq_len, batch_size)`. We don't use `batch_first` approach with `LSTM` so we need `seq_len` to be the first dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_src = read_corpus('./sanity_check_en_es_data/train_sanity_check.es', 'src')\n",
    "train_data_tgt = read_corpus('./sanity_check_en_es_data/train_sanity_check.en', 'tgt')\n",
    "train_data = list(zip(train_data_src, train_data_tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, list)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data_src), type(train_data_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pero,', 'qu', 'puedes', 'hacer?', 'Ests', 'en', 'el', 'medio', 'del', 'ocano.'] 10\n"
     ]
    }
   ],
   "source": [
    "print(train_data_src[0], len(train_data_src[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'But', 'what', 'can', 'you', 'do?', \"You're\", 'in', 'the', 'middle', 'of', 'the', 'ocean.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(train_data_tgt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = batch_iter(train_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sents, tgt_sents = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len of our batches is 5 (this is our batch_size)\n",
    "len(src_sents), len(tgt_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([22, 15, 10, 9, 7], [20, 21, 14, 14, 8])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(s) for s in src_sents], [len(s) for s in tgt_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pero,', 'qu', 'puedes', 'hacer?', 'Ests', 'en', 'el', 'medio', 'del', 'ocano.']\n"
     ]
    }
   ],
   "source": [
    "print(src_sents[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_lengths = [len(s) for s in src_sents]\n",
    "source_padded = model.vocab.src.to_input_tensor(src_sents, device=model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 5])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we may see that indeed 22 is the max len of source batch\n",
    "source_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 23, 3, 3, 3, 7, 6, 3, 14, 3]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check that our encoding is correct\n",
    "vocab.src.words2indices(src_sents)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3, 23,  3,  3,  3,  7,  6,  3, 14,  3,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_padded.t()[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our source data and can go through `encode` forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_padded = model.vocab.tgt.to_input_tensor(tgt_sents, device=model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so again we have 21 - max len in this batch\n",
    "# of size 5\n",
    "target_padded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `NMT` model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `init`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What kind of layers do we need? Well, we need:\n",
    "\n",
    "- we need `LSTM` layers for `encoder` and `decoder`; full `biLSTM` for `encoder` and one-directional `LSTMCell` for `decoder` (in case of decoder we have to incorporate attention into `LSTM`); \n",
    "- 2 `Embedding` layers for `encoder` and `decoder` (during training we supply gold target sequence to our `decoder`);\n",
    "- we also need a bunch of `Linear` layers with or without `bias`; their functions and sizes are pretty clear from `pdf`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few questions:\n",
    "    \n",
    "- what is the `input_size` for `encoder`? well, we use concatenated vector $\\bar{y}_t = [o_t; y_t]$ where $o_t$ has `hidden_size` (why is that? well, let's postpone this question untill attention section) and $y_t$ has `embedding_size`; so `input_size` is just the sum of these 2 sizes;\n",
    "- how do we use the fact that $W \\in \\mathbb{R}^{h \\times 2h}$? what should be the first and second dimensions of $W$? well, we use row input vector and transposed matrix: $xW^T$ (as stated in documentation) so dimension of $x$ should be the first dimension of $W^T$ or the second dimension of $W$; but things are a bit complicated here: shape of $W$ specified as `(out_features,in_features)`; the easiest way to escape these complications is to use named arguments: `in_features` is the size of input; `out_features` is the size of output (usually `hidden_size`);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `encode()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encode source data we need 3 steps:\n",
    "\n",
    "- step 1: get `embedding` from input tensor;\n",
    "- step 2: forward pass of the `LSTM`;\n",
    "- step 3: get `decoder` initial state (projections and concatenation);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  step 1: get `embedding` from input tensor;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step converts `source_padded` of shape `(seq_len, batch_size)` into `(seq_len, batch_size, embed_size)` which is `2` in our case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 5])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model.model_embeddings.source(source_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 5, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2: forward pass of the `LSTM`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward pass of the `LSTM` has a technical complication of packing. We don't discuss it here. What should be the shape after forward pass? Well we know that `LSTM` basically apply a few matrix multiplication so `embed_size` should change to `hidden_size`. But we have `biLSTM` so size should be multiplied by `2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_packed = pack_padded_sequence(X, lengths=source_lengths)\n",
    "enc_hiddens, (last_hidden, last_cell) = model.encoder(X_packed)\n",
    "enc_hiddens, _ = pad_packed_sequence(enc_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 5, 6])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_hiddens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to put `batch_size` back to the first place. Why is that? Well it looks like that in `step()` everything in the `batch first` shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_hiddens = enc_hiddens.permute(1, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 22, 6])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_hiddens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 3: `decoder` initial state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need some concatenations and projections (just a matrix multiplication or applying a linear layer without a bias). Let's look only at `last_hidden_cat` - the second operation is the same. \n",
    "\n",
    "We need to concatenate 2 tensors from `biLSTM` to get a high-dimensional tensor (`2*h`) and then project it back to low-dimensional (`h`). We need some way to get from the bidirectional `encoder` to the one directional `decoder` (`eq. 1-2` in `pdf`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the first dimension is for 2 states of biLSTM\n",
    "# we need to concatenate them\n",
    "last_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1965, -0.2512, -0.2357], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden[0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0804,  0.3910, -0.2768], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden[1, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_cat = torch.cat((last_hidden[0, :, :], last_hidden[1, :, :]), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1965, -0.2512, -0.2357, -0.0804,  0.3910, -0.2768],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_hidden_cat[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_decoder_hidden = model.h_projection(last_hidden_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we decreased the size back to h\n",
    "init_decoder_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cell_cat = torch.cat((last_cell[0, :, :], last_cell[1, :, :]), dim=1)\n",
    "init_decoder_cell = model.c_projection(last_cell_cat)\n",
    "dec_init_state = (init_decoder_hidden, init_decoder_cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Sanity Check for Question 1d: Encode\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "enc_hiddens Sanity Checks Passed!\r\n",
      "dec_init_state[0] Sanity Checks Passed!\r\n",
      "dec_init_state[1] Sanity Checks Passed!\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "All Sanity Checks Passed for Question 1d: Encode!\r\n",
      "--------------------------------------------------------------------------------\r\n"
     ]
    }
   ],
   "source": [
    "!python3 sanity_check.py 1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `decode()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 4 steps:\n",
    "\n",
    "- step 1: apply the attention projection layer;\n",
    "- step 2: construct tensor `Y` of target sentences;\n",
    "- step 3: iterate over the time dimension of `Y`;\n",
    "- step 4: reshape `combined_outputs`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all what arguments do we have? \n",
    "\n",
    "- `enc_hiddens` - this is just an output of our `encode` model, so these are outputs for *all* timesteps - it looks like we need all of them; the only thing we do - permute 2 first dimensions so the final shape is `(b, src_len, h*2)` (see explanation above);\n",
    "- `enc_masks` - we use only as an argument to `step()` and explain it later;\n",
    "- `dec_init_state` - that's again we compute in our `encoder` and it contains a tuple `(init_decoder_hidden, init_decoder_cell)`;\n",
    "- `target_padded` - this is out gold target data of shape `(seq_len, batch_size)`; again, we don't use batch first approach in our `decode` `LSTM`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initial operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chop of the <END> token for max length sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 5])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  3, 49,  2,  0],\n",
       "        [37, 52, 27,  3,  2],\n",
       "        [ 0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 is index for </s>\n",
    "target_padded.t()[:, -5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3,  3,  3, 49,  2],\n",
       "        [ 3, 37, 52, 27,  3],\n",
       "        [ 0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we may see that indeed the 2nd row is of max len 21\n",
    "# and now it ends with 3, not 2\n",
    "# so we removed </s> from it\n",
    "target_padded[:-1].t()[:, -5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_padded = target_padded[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### misc operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the decoder state (hidden and cell)\n",
    "dec_state = dec_init_state\n",
    "\n",
    "# initialize previous combined output vector o_{t-1} as zero\n",
    "batch_size = enc_hiddens.size(0)\n",
    "o_prev = torch.zeros(batch_size, model.hidden_size, device=model.device)\n",
    "\n",
    "# initialize a list we will use to collect the combined output o_t on each step\n",
    "combined_outputs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1: apply the attention projection layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to deal with the difference between `encoder` and `decoder`, so we have to reduce dimensionality of `enc_hiddens` (`2*h`). We need this to get our multiplicative attention (`eq. 7` in `pdf`): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$e_{ti} = (h^{dec}_t)^T W_{attProj} h^{enc}_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 22, 6])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_hiddens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_hiddens_proj = model.att_projection(enc_hiddens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 22, 3])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# last dimension is reduced from 6 to 3\n",
    "enc_hiddens_proj.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2: construct tensor `Y` of target sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just embed the input to our `decoder`: for shape `(seq_len, batch_size)` we get `(seq_len, batch_size, embed_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 5])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for some reason we removed last symbol\n",
    "# and batch_size is the 2nd dimension\n",
    "target_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = model.model_embeddings.target(target_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding size is the same for encoder and for decoder\n",
    "model.model_embeddings.embed_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 5, 2])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 3: iterate over the time dimension of `Y`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main loop of our decoder like described in `eq. 5` of our `pdf`:\n",
    "\n",
    "$$h^{dec}_t, c^{dec}_t = decoder(\\bar{y}_t, h^{dec}_{t-1}, c^{dec}_{t-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more time - `split()` means that we just turn `(tgt_len, b, e)` tensor into tuple of `(1, b, e)` tensors. We use `LSTMCell`, not `LSTM` so we have to operate on one input (not on a sequence), but we're still able to process a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(np.arange(10*3*2).reshape(10, 3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_split = torch.split(x, 1, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, 10)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_split), len(x_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 2])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_split[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_split[0].squeeze().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All magic is incorporated in `step()` function which basically is the `forward` method of the `decoder`. The only additional argument we have to supply is $\\bar{y}_t$. We construct it as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_0 = torch.split(Y, 1, dim=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([20, 5, 2]), torch.Size([1, 5, 2]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape, Y_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_0 = Y_0.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_prev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ybar_0 = torch.cat((Y_0, o_prev), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ybar_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.2890, -0.7240], grad_fn=<SliceBackward>), tensor([0., 0., 0.]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_0[0, :], o_prev[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2890, -0.7240,  0.0000,  0.0000,  0.0000], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ybar_0[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 4: stack `combined_outputs` from a list to a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to stack list of `o_t` (of shape `(batch_size, hidden_size)`) to get combine output of our `decoder`. We need `seq_len` to be the first dimension so we use `torch.stack(combined_outputs, dim=0)`. So in this case we don't need `batch first` order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq_len is 10 in our case\n",
    "np.random.seed(42)\n",
    "x = [torch.Tensor(np.random.randn(2, 3)) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.stack(x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2, 3])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so seq_len is i fact the first dimension\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.stack(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 3])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Running Sanity Check for Question 1e: Decode\n",
      "--------------------------------------------------------------------------------\n",
      "combined_outputs Sanity Checks Passed!\n",
      "--------------------------------------------------------------------------------\n",
      "All Sanity Checks Passed for Question 1e: Decode!\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!python3 sanity_check.py 1e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `step()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So finally we have to write a function that computes attention. First of all let's understand how it should be computed:\n",
    "\n",
    "- we compute attention scores $e_t = (h^{dec}_t W_{attProj} h^{enc}_i)$; so we need: (a) current decoder hidden state $h^{dec}_t$ - we compute it using `decoder` `forward` pass, we need `dec_state` and `Ybar_t`; (b) `enc_hiddens_proj` - it's an argument;\n",
    "- we compute attention weights $\\alpha_t = softmax(e_t)$;\n",
    "- now we may compute attention output $a_t = \\sum{\\alpha_{ti} h^{enc}_i}$; we need hidden states of encoder `enc_hiddens`; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at arguments of `step()`:\n",
    "\n",
    "- `Ybar_t` - we compute in `decode` as concatenation of `Y_t` and `o_t`;\n",
    "- `dec_state` - that's a result of the `forward` pass od our `decoder`;\n",
    "- `enc_hiddens` - that's a result of `encode()`;\n",
    "- `enc_hiddens_proj` - we compute this in `decode` by applying `Linear` layer;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at `torch.bmm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(10, 3, 4)\n",
    "mat2 = torch.randn(10, 4, 5)\n",
    "res = torch.bmm(input, mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 5])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 5])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.bmm(mat2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the forward pass. \n",
    "\n",
    "Then we have to multiply `dec_hidden` of shape `(batch_size, hidden_size)` and `enc_hiddens_proj` of shape `(batch_size, seq_len, hidden_size)`. What is the shape of `e_t`? It should be `(batch_size, seq_len)`. In other words we multiply matrix of size `(seq_len, hidden_size)` and vector of size `hidden_size` for each row in a batch.\n",
    "\n",
    "How do we get this result using `torch.bmm`? First of all we have to make `dec_hidden` a `3D` tensor using `unsqueeze()`. We then multiply `enc_hiddens_proj` and `dec_hidden` in this order. Finally we should remove last dimension to get `2D` tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_state = model.decoder(Ybar_0, dec_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_hidden, dec_cell = dec_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 22, 3])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_hiddens_proj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3, 1])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_hidden.unsqueeze(dim=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_0 = torch.bmm(enc_hiddens_proj, dec_hidden.unsqueeze(dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 22, 1])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_0 = e_0.squeeze(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 22])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e_0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need to apply `softmax` to `e_t`. What `dimension` should we use? Well we need to get weights to combine `encoder` hidden states: $a_t = \\sum{\\alpha_{ti} h^{enc}_i}$. So we need to apply `softmax` to each row in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_0 = F.softmax(e_t, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 22])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so we in fact have weights that sum to 1\n",
    "torch.sum(alpha_0[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to get this attention output vector using formula above. So we need to use `torch.bmm` yet again. So we have to multiply `alpha_t` of shape `(b, seq_len)` and `enc_hiddens` of shape `(b, seq_len, 2h)` to get `(b, 2h)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 22, 6])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_hiddens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 22])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_0.unsqueeze(dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 6])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(alpha_0.unsqueeze(dim=1), enc_hiddens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_0 = torch.bmm(alpha_0.unsqueeze(dim=1), enc_hiddens).squeeze(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we have to compute combined output $o_t$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$u_t = [a_t; h^{dec}_t]$$\n",
    "$$v_t = W_u u_t$$\n",
    "$$o_t = dropout(tanh(v_t))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 6]), torch.Size([5, 3]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_0.shape, dec_hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_0 = torch.cat((a_0, dec_hidden), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.1954, -0.1677, -0.1786, -0.0671,  0.3494, -0.2230],\n",
       "        grad_fn=<SliceBackward>),\n",
       " tensor([-0.0669, -0.0143,  0.0723], grad_fn=<SliceBackward>))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_0[0, :], dec_hidden[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1954, -0.1677, -0.1786, -0.0671,  0.3494, -0.2230, -0.0669, -0.0143,\n",
       "         0.0723], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_0[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything else is straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_0 = model.combined_output_projection(U_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "O_0 = model.dropout(torch.tanh(V_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 3])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "O_0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Running Sanity Check for Question 1f: Step\n",
      "--------------------------------------------------------------------------------\n",
      "dec_state[0] Sanity Checks Passed!\n",
      "dec_state[1] Sanity Checks Passed!\n",
      "combined_output  Sanity Checks Passed!\n",
      "e_t Sanity Checks Passed!\n",
      "--------------------------------------------------------------------------------\n",
      "All Sanity Checks Passed for Question 1f: Step!\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!python3 sanity_check.py 1f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this completes debugging of our `nmt_model`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
