{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vocab import *\n",
    "from utils import *\n",
    "from model_embeddings import *\n",
    "from nmt_model import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not easy to debug the model - we don't have tests and shape hints, and also it uses new for this course `conv1D` and brand new `highway` layers. First of all - what are we going to achieve? We don't use word embeddings in a usual way. Instead we build them in the following steps (see `figure 2` in `pdf`):\n",
    "\n",
    "- we encode chars in a word using char vocabulary (and this is already done);\n",
    "- we use `Embedding` layer for chars (not for words); we then use `conv1D` and `highway` layers (that are built from scratch in separate files); we do this in `ModelEmbeddings` class;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 issues with A5 (partially mentioned above):\n",
    "\n",
    "- we don't have shape hints and have to figure out these shapes; it's easier for me to do using actual data, not theoretically; we may also create (missing) tests using these data;\n",
    "- we also have to build all classes in the reverse order and that's also not helpful; we may start from creating data, then encode them, turn them into char embedding and so on;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all let's get some raw data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muchas gracias Chris. Y es en verdad un gran honor tener la oportunidad de venir a este escenario por segunda vez. Estoy extremadamente agradecido.\r\n",
      "He quedado conmovido por esta conferencia, y deseo agradecer a todos ustedes sus amables comentarios acerca de lo que tena que decir la otra noche.\r\n",
      "Y digo eso sinceramente, en parte porque -- (Sollozos fingidos) -- lo necesito!  Pnganse en mi posicin!\r\n"
     ]
    }
   ],
   "source": [
    "!head -3 './en_es_data/train_tiny.es'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_src = read_corpus('./en_es_data/train_tiny.es', source='src')\n",
    "train_data_tgt = read_corpus('./en_es_data/train_tiny.en', source='tgt')\n",
    "train_data = list(zip(train_data_src, train_data_tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = batch_iter(train_data, batch_size=train_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sents, tgt_sents = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25, 24, 18]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(src_sents[i]) for i in range(len(src_sents))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 25)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(src_sents[0]), len(src_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['He', 'quedado', 'conmovido', 'por', 'esta'], ['Muchas', 'gracias', 'Chris.', 'Y', 'es'], ['Y', 'digo', 'eso', 'sinceramente,', 'en']]\n"
     ]
    }
   ],
   "source": [
    "print([src_sents[i][:5] for i in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab.load('vocab_tiny_q1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 132)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.src), len(vocab.tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our src vocab_entry contains the same \n",
    "# predefined char_list\n",
    "len(vocab.src.char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_padded_chars = vocab.src.to_input_tensor_char(src_sents, device=torch.device('cpu')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 3, 21])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_padded_chars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_padded_chars_resh = source_padded_chars.contiguous().view(3, 25, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 11, 34,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_padded_chars_resh[0, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is encoding of `He` (together with `{` and `}` symbols and padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{', 'H', 'e', '}', '<pad>']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[vocab.src.id2char[i] for i in source_padded_chars_resh[0, 0, :5].numpy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know this shape from previous analysis. Here `21` is our predefined `max_seq_len`, `25` is `max_word_len` in our batch (see above) and `3` is `batch_size`. But why do we need this shape? It looks like `source_padded_chars` is an input to our char `Embedding` layer. Let's try to figure out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ModelEmbeddings`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### char `Embedding` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First layer in `ModelEmbeddings` is `Embedding` layer. To create this layer we need 2 parameters:\n",
    "\n",
    "- `num_embeddings` - this is the length of vocabulary, in our case char vocabulary; in our case `96` (see below);\n",
    "- `embedding_dim` - is predefined and equal to `50` (which is lower than `256` in assignment `4`); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We actually have predefined char vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_entry = VocabEntry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['A', 'B', 'C', 'D', 'E'], 92)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_entry.char_list[:5], len(vocab_entry.char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<pad>', 0), ('{', 1), ('}', 2), ('<unk>', 3), ('A', 4)] 96\n"
     ]
    }
   ],
   "source": [
    "# so we have 4 additional symbols in addition to char_list\n",
    "print(list(vocab_entry.char2id.items())[:5], len(vocab_entry.char2id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should be the input to this layer? It looks like `Embedding` just adds one dimension - from docs:\n",
    "\n",
    "- Output: `(*, H)`, where `(*)` is the input shape and `H` `char_embedding_dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "embed = nn.Embedding(num_embeddings=len(vocab.src.char2id),\n",
    "                     embedding_dim=50,\n",
    "                     padding_idx=vocab.src.char2id['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embed = embed(source_padded_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 3, 21, 50])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is in fact the same shape as input has plus `char_embedding_dim`. Let's check that this is our word `He`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0780,  0.5258, -0.4880,  1.1914, -0.8140],\n",
       "        [-0.1678,  1.6433,  2.0071, -1.2531,  1.1189],\n",
       "        [-0.6106,  1.0629,  1.2222,  0.7719, -1.2797],\n",
       "        [ 0.6408,  0.5832,  1.0669, -0.4502, -0.1853]],\n",
       "       grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_embed[0, 0, :4, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = embed.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 50])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 11, 34,  2])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_padded_chars_resh[0, 0, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0780,  0.5258, -0.4880,  1.1914, -0.8140],\n",
       "        [-0.1678,  1.6433,  2.0071, -1.2531,  1.1189],\n",
       "        [-0.6106,  1.0629,  1.2222,  0.7719, -1.2797],\n",
       "        [ 0.6408,  0.5832,  1.0669, -0.4502, -0.1853]],\n",
       "       grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W[source_padded_chars_resh[0, 0, :4].numpy(), :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we still don't know why do we use such shape for our input. Next stop - `conv1D` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conv1D and max_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is probably the most difficult place for us to undestand all the shapes. Let's start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conv1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What shape should be an input to `conv1D`? As specified in docs: `(N, C, L)` where `C` is number of (input) channels, `L` is sequence length. Fortunately we have a hint what this means in our case. In `pdf` specified that input to `conv1D` should be of shape: $(e_{char}, m_{word})$.  In our case `(50, 21)`. So we have to combine the first two dimensions and swap the last two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is consistent with what specified in the book: \n",
    "\n",
    "- `number of channels` is the length of the vector (in our case `embed_size` `50`);\n",
    "- `seq_len` is the length of char sequence (in our case `max_word_len` `21`);\n",
    "\n",
    "If we're looking for an analogy with vision: characters are our pixels and instead of 3 channels `RGB` we have `embed_size` channels.\n",
    "\n",
    "The input size to `conv1D` layer is also specified in the book: *`Conv1d` layers we will use require the data tensors to have the batch on the 0th dimension, channels on the 1st dimension, and sequence length on the 2nd.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 3, 21, 50])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, seq_len, n_channels  = x_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 21)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_channels, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_embed_resh = x_embed.view(-1, n_channels, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([75, 50, 21])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_embed_resh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create `conv1D` layer. We need 3 parameters:\n",
    "\n",
    "- `in_channels` - this is `n_channels` in `x_embed` (the same as `char_embed_size`, `50`);\n",
    "- `out_channels` (the same as `num_filters`) - this is an important parameter; let's see where it's used: we create `embedding` before supply data to `LSTM`; we create it with `ModelEmbeddings(embed_size, vocab.src)`; this is actually **`word_embedding_size`** and it's used as a `num_filters`; the reason for that - we use all this steps with convolution etc. just to create word embedding as specified above; we use `256` as `word_embedding_size` (as specified in `pdf`);\n",
    "- `kernel_size` - predefined in `pdf` and equal to `5`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "char_embed_size = 50\n",
    "num_filters = 256\n",
    "kernel_size = 5\n",
    "conv1D = nn.Conv1d(in_channels=char_embed_size,\n",
    "                   out_channels=num_filters,\n",
    "                   kernel_size=kernel_size,\n",
    "                   bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_conv1D = conv1D(x_embed_resh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([75, 256, 17])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_conv1D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shape is not suprising:\n",
    "\n",
    "- `n_channels` switched from `in_channels` to `out_channels`; \n",
    "- to compute an output size of applying a filter we use standard formula (from C4W1L05 cs230):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{n + 2p - f}{s} + 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we don't use padding and\n",
    "# use default stride = 1\n",
    "(21 + 0 - 5) / 1 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### max_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is `max_pool`. It looks like it works with 3D tensors like `conv1D` tensor. And it changes only the last dimension:\n",
    "\n",
    "- Input: $(N, C, L_{in})$\n",
    "- Output: $(N, C, L_{out})$\n",
    "\n",
    "Here's the formula for this last dimension (`stride == k` by default):\n",
    "\n",
    "$$L_{out} = \\frac{L_{in} - k}{k} + 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(15 - 3) / 3 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.MaxPool1d(kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "x_in = torch.randn(2, 3, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 15])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784, -1.2345, -0.0431, -1.6047,\n",
       "        -0.7521,  1.6487, -0.3925, -1.4036, -0.7279, -0.5594, -0.7688])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_in[0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out = m(x_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9269,  0.6784, -0.0431,  1.6487, -0.5594])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_out[0, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that this is the correct result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9269,  0.6784, -0.0431,  1.6487, -0.5594])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([torch.max(x_in[0, 0, i:i+3]) for i in range(0, 15, 3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should be the `kernel_size` in our case? This is specified in `pdf`. Output of `x_conv` has shape `(batch_size, word_embed_size, max_word_len - k + 1)` or `(3, 256, 17)` in our case. After `max_pool` it will be just first 2 dimensions `((batch_size, word_embed_size)`.\n",
    "\n",
    "In terms of `pdf`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$x_{conv} \\in \\mathbb{R}^{e_{word} \\times (m_{word} - k + 1)}$$\n",
    "$$x_{conv-out} \\in \\mathbb{R}^{e_{word}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is that? Well it's the most popular pooling over time when the window of the filter goes via `seq_len`. What should be the `kernel_size`? If we need to remove the 2nd direction the size should be equal to its dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool = nn.MaxPool1d(kernel_size=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_conv_out = max_pool(F.relu(x_conv1D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([75, 256, 1])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_conv_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_conv_out = x_conv_out.squeeze(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([75, 256])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_conv_out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## highway network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last step of the part 1 is `highway` network. It's much easier to understand than `conv1D`. Shape is not changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embed_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "proj = nn.Linear(in_features=word_embed_size, out_features=word_embed_size, bias=True)\n",
    "gate = nn.Linear(in_features=word_embed_size, out_features=word_embed_size, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_proj = F.relu(proj(x_conv_out))\n",
    "x_gate = torch.sigmoid(gate(x_conv_out))\n",
    "x_highway = x_gate * x_proj + (1 - x_gate) * x_conv_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([75, 256])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_highway.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the analysis of the part 1 of A5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Running Sanity Check for Question 1h: Highway\n",
      "--------------------------------------------------------------------------------\n",
      "Sanity Check Passed for Question 1h: Highway!\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "! python3 sanity_check.py 1h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(x_conv_out, './sanity_check_en_es_data/1h_x_conv_out.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(x_highway, './sanity_check_en_es_data/1h_x_highway.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
