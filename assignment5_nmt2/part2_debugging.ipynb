{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from char_decoder import *\n",
    "from sanity_check import *\n",
    "from vocab import *\n",
    "from utils import *\n",
    "from model_embeddings import *\n",
    "from nmt_model import *\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 (a) `__init__` of  `CharDecoder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize `CharDecoder` the only thing we have to get right is shapes:\n",
    "\n",
    "- `LSTM` works with embedded vectors, so its `input_size` should be equal to this `embedding size` - in our case `char_embedding_size` (we basically define matrices of `LSTM` that should work on input vectors);\n",
    "- our `Linear` layer should project on $V_{char}$ space to get distribution over chars; as we know its easier to work with linear layers in terms of `in_features` and `out_features`, not directly with matrix shapes; so as an input we get vectors from `LSTM` of `hidden_size` and `out_features` or $V_{char}$ (or length of appropiate vocabulary);\n",
    "- finally `Embedding` layer should translate vectors from $V_{char}$ to $e_{char}$ space;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vocab = DummyVocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(char_vocab.char2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('<pad>', 0), ('{', 1), ('}', 2), ('<unk>', 3), ('a', 4), ('b', 5), ('c', 6), ('d', 7), ('e', 8), ('f', 9), ('g', 10), ('h', 11), ('i', 12), ('j', 13), ('k', 14), ('l', 15), ('m', 16), ('n', 17), ('o', 18), ('p', 19), ('q', 20), ('r', 21), ('s', 22), ('t', 23), ('u', 24), ('v', 25), ('w', 26), ('x', 27), ('y', 28), ('z', 29)])\n"
     ]
    }
   ],
   "source": [
    "print(char_vocab.char2id.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HIDDEN_SIZE, EMBED_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = CharDecoder(\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    char_embedding_size=EMBED_SIZE+1,\n",
    "    target_vocab=char_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharDecoder(\n",
       "  (charDecoder): LSTM(4, 3)\n",
       "  (char_output_projection): Linear(in_features=3, out_features=30, bias=True)\n",
       "  (decoderCharEmb): Embedding(30, 4, padding_idx=0)\n",
       ")"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\r\n",
      "Running Sanity Check for Question 2a: CharDecoder.__init__()\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "Sanity Check Passed for Question 2a: CharDecoder.__init__()!\r\n",
      "--------------------------------------------------------------------------------\r\n"
     ]
    }
   ],
   "source": [
    "!python3 sanity_check.py 2a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 (b) `forward()` function of `CharDecoder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of an output is specified in this case: `(length, batch, self.vocab_size)`. Let's use the example from `sanity_check.py` and check the shape:\n",
    "\n",
    "- `sequence_length` is indeed `4`;\n",
    "- `batch_size` is `5`;\n",
    "- finally `self.vocab_size` is `30` (see above);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 4\n",
    "inpt = torch.zeros(sequence_length, BATCH_SIZE, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, (dec_hidden1, dec_hidden2) = decoder.forward(inpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5, 30])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\r\n",
      "Running Sanity Check for Question 2b: CharDecoder.forward()\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "Sanity Check Passed for Question 2b: CharDecoder.forward()!\r\n",
      "--------------------------------------------------------------------------------\r\n"
     ]
    }
   ],
   "source": [
    "!python3 sanity_check.py 2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 (c) `train forward()` of `CharDecoder`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we need to compute `loss` to train `CharDecoder`. It's not quite clear why we use this form of the loss but at least it's specified in `pdf`.\n",
    "\n",
    "First of all:\n",
    "\n",
    "- if `char_sequence` is `<START>,m,u,s,i,c,<END>` we need to remove the last symbol to feed it into `forward()`;\n",
    "- we also need to create the `target` sequence - in this case we have to remove the first symbol;\n",
    "- we also have to use `forward()` method to get logits `s`;\n",
    "\n",
    "And now we have to use `nn.CrossEntropyLoss` but somehow modify it to account:\n",
    "\n",
    "- for the fact that we use `sum`, not `average` across the batch; \n",
    "- we also have to ignore `pad_symbol`;\n",
    "- finally we have to reshape our logits and target to feed them into `nn.CrossEntropyLoss`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\r\n",
      "Running Sanity Check for Question 2c: CharDecoder.train_forward()\r\n",
      "--------------------------------------------------------------------------------\r\n",
      "Sanity Check Passed for Question 2c: CharDecoder.train_forward()!\r\n",
      "--------------------------------------------------------------------------------\r\n"
     ]
    }
   ],
   "source": [
    "! python3 sanity_check.py 2c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test on tiny dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get batch of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use our dataset to debug `CharDecoder` like in the first part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you so much, Chris. And it's truly a great honor to have the opportunity to come to this stage twice; I'm extremely grateful.\r\n",
      "I have been blown away by this conference, and I want to thank all of you for the many nice comments about what I had to say the other night.\r\n",
      "And I say that sincerely, partly because (Mock sob) I need that.  Put yourselves in my position.\r\n"
     ]
    }
   ],
   "source": [
    "!head -3 './en_es_data/train_tiny.en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_src = read_corpus('./en_es_data/train_tiny.es', source='src')\n",
    "train_data_tgt = read_corpus('./en_es_data/train_tiny.en', source='tgt')\n",
    "train_data = list(zip(train_data_src, train_data_tgt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = batch_iter(train_data, batch_size=train_batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sents, tgt_sents = next(it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we probably need `tgt_sents`, not `src_sents`. Again these are still `list[list[str]]`, sorted and enclosed in `<s>` and `</s>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 26, 20]"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(tgt_sents[i]) for i in range(len(tgt_sents))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s>', 'I', 'have', 'been', 'blown'], ['<s>', 'Thank', 'you', 'so', 'much,'], ['<s>', 'And', 'I', 'say', 'that']]\n"
     ]
    }
   ],
   "source": [
    "print([tgt_sents[i][:5] for i in range(3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['say', 'the', 'other', 'night.', '</s>'], ['twice;', \"I'm\", 'extremely', 'grateful.', '</s>'], ['yourselves', 'in', 'my', 'position.', '</s>']]\n"
     ]
    }
   ],
   "source": [
    "print([tgt_sents[i][-5:] for i in range(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab.load('vocab_tiny_q2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 32)"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the sizes of vocabs are even smaller than before\n",
    "len(vocab.src), len(vocab.tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<pad>', 0), ('<s>', 1), ('</s>', 2), ('<unk>', 3), ('to', 4)]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.tgt.word2id.items())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# that's the same list as before\n",
    "len(vocab.tgt.char_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab.tgt.char2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encode characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part we encoded our words using `to_input_tensor_char()`. What are we going to do now? It seems we still need this encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_padded_chars = vocab.tgt.to_input_tensor_char(tgt_sents, device=torch.device('cpu')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 21])"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_padded_chars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_padded_chars_resh = target_padded_chars.reshape(3, 32, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 32, 21])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_padded_chars_resh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 90, 48, 91,  2],\n",
       "        [ 1, 12,  2,  0,  0],\n",
       "        [ 1, 37, 30, 51, 34],\n",
       "        [ 1, 31, 34, 34, 43],\n",
       "        [ 1, 31, 41, 44, 52]])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_padded_chars_resh[0, :5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['{', '<', 's', '>', '}'],\n",
       " ['{', 'I', '}', '<pad>', '<pad>'],\n",
       " ['{', 'h', 'a', 'v', 'e'],\n",
       " ['{', 'b', 'e', 'e', 'n'],\n",
       " ['{', 'b', 'l', 'o', 'w']]"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[vocab.tgt.id2char[i] for i in row] for row in target_padded_chars_resh.numpy()[0, :5, :5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CharDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `init` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's one more time verify that this is correct shape:\n",
    "\n",
    "- `Embedding` layer should have input size of our target char vocab (`96` - see above) and make an embedding of size `7` - specified below; \n",
    "- `LSTM` should transfer these embeddings into space of `hidden_size` (`5` - specified below);\n",
    "- finally `Linear` layer should project these vectors back to space of size `96` (to get prediction of the next character);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_decoder = CharDecoder(hidden_size=5, char_embedding_size=7, target_vocab=vocab.tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharDecoder(\n",
       "  (charDecoder): LSTM(7, 5)\n",
       "  (char_output_projection): Linear(in_features=5, out_features=96, bias=True)\n",
       "  (decoderCharEmb): Embedding(96, 7, padding_idx=0)\n",
       ")"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now one of the most difficult moments - we have to go throw the forward pass of `CharDecoder`. \n",
    "\n",
    "Input shape to `forward()` in `CharDecoder` is specified as `(length, batch)`. The question is  - how can we get this shape from `target_padded_chars` which is `3D` tensor. It looks like we have to reshape it. It's not clear why should we remove the first elements right now. That's a code inside `forward()` in `nmt_model.py`.\n",
    "\n",
    "There are 2 main questions:\n",
    "\n",
    "- why should we remove `<s>` here if we'are told in `train_forward` that `char_sequence` corresponds to the sequence `x_1 ... x_{n+1}` from the handout (e.g., `<START>,m,u,s,i,c,<END>`); in other word **including** `<s>`;\n",
    "- by removing it like `target_padded_chars[1:]` we actually break our examples: instead of `<s> I have been blown ...` we get `been blown ...` (but maybe we just don't have to reshape after this operation);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_word_len = target_padded_chars.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_word_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_chars = target_padded_chars[1:].view(-1, max_word_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([93, 21])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_chars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 93])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so this is looks like (length, batch) specified in train_forward\n",
    "target_chars.t().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did we remove?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 21])"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_padded_chars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 90, 48, 91,  2],\n",
       "        [ 1, 31, 34, 34, 43],\n",
       "        [ 1, 31, 54,  2,  0],\n",
       "        [ 1, 30, 43, 33,  2],\n",
       "        [ 1, 49, 44,  2,  0]])"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_padded_chars[:5, 0, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 31, 34, 34, 43],\n",
       "        [ 1, 31, 54,  2,  0],\n",
       "        [ 1, 30, 43, 33,  2],\n",
       "        [ 1, 49, 44,  2,  0],\n",
       "        [ 1, 44, 35,  2,  0]])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_padded_chars[1:][:5, 0, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 90, 48, 91,  2],\n",
       "        [ 1, 12,  2,  0,  0],\n",
       "        [ 1, 37, 30, 51, 34],\n",
       "        [ 1, 31, 34, 34, 43],\n",
       "        [ 1, 31, 41, 44, 52]])"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_padded_chars.view(3, 32, 21)[0, :5, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 3, 21])"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_padded_chars[1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 31, 34, 34, 43],\n",
       "        [ 1, 31, 41, 44, 52],\n",
       "        [ 1, 30, 52, 30, 54],\n",
       "        [ 1, 31, 54,  2,  0],\n",
       "        [ 1, 49, 37, 38, 48]])"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_padded_chars[1:].view(3, 31, 21)[0, :5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `forward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 21])"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_padded_chars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_sequence = target_padded_chars.view(21, 3 * 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 96])"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = char_sequence[:-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 96])"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What exactly did we remove? That's not `start` and `end` symbols. Not quite clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_sequence[-1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 30, 52, 30, 54,  2,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 31, 54,  2,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 49, 37,\n",
       "        38, 48,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         1, 32, 44, 43, 35, 34, 47, 34, 43, 32, 34, 66,  2,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  1, 30, 43])"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_sequence[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, dec_hidden = char_decoder(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 96, 96])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# that's in fact (length, batch, self.vocab_size)\n",
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = char_sequence[1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 96])"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `CrossEntropy` loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to compute `CrossEntropy` loss? Well in A2 we saw that: $CE(y, \\hat{y}) = -\\sum{y_w log(\\hat{y}_w)}$ and in case we use `one-hot-encoded` vectors this equals $-log(\\hat{y}_o)$ where $o$ is the correct class. Here $\\hat{y}_w$ should be probabilities - in other words scores after softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what we may see in the documentation to `pytorch` (here $x$ - one training examples):\n",
    "\n",
    "$$loss(x, class) = -log(\\frac{exp(x(class))}{\\sum{exp(x_j)}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get `loss` for the batch we have to average over training examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to identify options that shoud be specified in `CrossEntropy`:\n",
    "\n",
    "- `ignore_index (int, optional)` – specifies a target value that is ignored and does not contribute to the input gradient;\n",
    "- `reduction (string, optional)` – Specifies the reduction to apply to the output: `'none' | 'mean' | 'sum'`; we need `sum`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see at an example of `CrossEntropy` loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "input = torch.randn(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3367,  0.1288,  0.2345,  0.2303, -1.1229],\n",
       "        [-0.1863,  2.2082, -0.6380,  0.4617,  0.2674],\n",
       "        [ 0.5349,  0.8094,  1.1103, -1.6898, -0.9890]])"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "target_ex = torch.randint(low=0, high=5, size=(3,), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 1])"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9635)"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(input, target_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we compute this amount manually?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.2642289 , 0.52834964, 2.2464635 ], dtype=float32)"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(input[range(input.numpy().shape[0]), target_ex.numpy()].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.3863764, 13.350886 ,  7.5455084], dtype=float32)"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.exp(input.numpy()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.exp(input[range(input.shape[0]), target_ex.numpy()].numpy()) / np.sum(np.exp(input.numpy()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23470862, 0.03957412, 0.29772195], dtype=float32)"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2347086066989303"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.2642289 / 5.3863764"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9635286"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(-np.log(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may do this even easier using `softmax`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.softmax(input, dim=1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.25997174, 0.21117601, 0.23470864, 0.23374145, 0.06040223],\n",
       "       [0.06216823, 0.68155295, 0.03957412, 0.11884615, 0.09785859],\n",
       "       [0.22626513, 0.29772198, 0.40225777, 0.02445913, 0.04929599]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = s[range(input.numpy().shape[0]), target_ex.numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23470864, 0.03957412, 0.29772198], dtype=float32)"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9635285"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(-np.log(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute loss we have to provide `input` and `target`: `output = loss(input, target)`:\n",
    "\n",
    "- `input` has shape `(batch_size, n_classes)`, so each row contains unnormalized logits;\n",
    "- `target` has shape `(batch_size,)` and contains correct class for each training example in the batch;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 96, 96])"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 96])"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 30, 52, 30, 54,  2,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 31, 54,  2,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 49, 37,\n",
       "        38, 48,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         1, 32, 44, 43, 35, 34, 47, 34, 43, 32, 34, 66,  2,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  1, 30, 43])"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1920, 96])"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.view(-1, score.shape[-1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1920])"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss(reduction='sum',\n",
    "                           ignore_index=vocab.tgt.char2id['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2022.9661, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(score.view(-1, score.shape[-1]), target.contiguous().view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our debugging of the part 2. Everything else is out of scope. This assignment is too big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what did we crop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review our steps before we crop our data:\n",
    "\n",
    "- get a batch of examples as `list[str]]`;\n",
    "- encode them using `char` vocabulary;\n",
    "- reshape them in `(max_word_len, batch_size)`, in our case `(21, 96)`; basically we stack them vertically;\n",
    "- that's the place when we crop them into `input` and `target`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<s>', 'I', 'have', 'been', 'blown', 'away', 'by', 'this'],\n",
       " ['<s>', 'Thank', 'you', 'so', 'much,', 'Chris.', 'And', \"it's\"],\n",
       " ['<s>', 'And', 'I', 'say', 'that', 'sincerely,', 'partly', 'because']]"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tgt_sents[i][:8] for i in range(len(tgt_sents))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 26, 20]"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(tgt_sents[i]) for i in range(len(tgt_sents))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_padded_chars = vocab.tgt.to_input_tensor_char(tgt_sents, device=torch.device('cpu')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 21])"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_padded_chars.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we get our examples back?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1, 90, 48, 91,  2,  0,  0,  0,  0,  0],\n",
       "         [ 1, 12,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 1, 37, 30, 51, 34,  2,  0,  0,  0,  0],\n",
       "         [ 1, 31, 34, 34, 43,  2,  0,  0,  0,  0],\n",
       "         [ 1, 31, 41, 44, 52, 43,  2,  0,  0,  0]],\n",
       "\n",
       "        [[ 1, 90, 48, 91,  2,  0,  0,  0,  0,  0],\n",
       "         [ 1, 23, 37, 30, 43, 40,  2,  0,  0,  0],\n",
       "         [ 1, 54, 44, 50,  2,  0,  0,  0,  0,  0],\n",
       "         [ 1, 48, 44,  2,  0,  0,  0,  0,  0,  0],\n",
       "         [ 1, 42, 50, 32, 37, 66,  2,  0,  0,  0]],\n",
       "\n",
       "        [[ 1, 90, 48, 91,  2,  0,  0,  0,  0,  0],\n",
       "         [ 1,  4, 43, 33,  2,  0,  0,  0,  0,  0],\n",
       "         [ 1, 12,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "         [ 1, 48, 30, 54,  2,  0,  0,  0,  0,  0],\n",
       "         [ 1, 49, 37, 30, 49,  2,  0,  0,  0,  0]]])"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_padded_chars.reshape(3, 32, 21)[:, :5, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['{', '<', 's', '>', '}', '<pad>', '<pad>', '<pad>'],\n",
       " ['{', 'I', '}', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'],\n",
       " ['{', 'h', 'a', 'v', 'e', '}', '<pad>', '<pad>'],\n",
       " ['{', 'b', 'e', 'e', 'n', '}', '<pad>', '<pad>'],\n",
       " ['{', 'b', 'l', 'o', 'w', 'n', '}', '<pad>']]"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[vocab.tgt.id2char[i] for i in target_padded_chars.reshape(3, 32, 21).numpy()[0, j, :8]]\n",
    " for j in range(5)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_sequence = target_padded_chars.view(21, 3 * 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 96])"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_sequence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 90, 48, 91,  2,  0,  0,  0,  0,  0],\n",
       "        [ 1, 12,  2,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1, 37, 30, 51, 34,  2,  0,  0,  0,  0],\n",
       "        [ 1, 31, 34, 34, 43,  2,  0,  0,  0,  0],\n",
       "        [ 1, 31, 41, 44, 52, 43,  2,  0,  0,  0]])"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_sequence.reshape(96, 21)[:5, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 90, 48, 91,  2,  0,  0,  0,  0,  0],\n",
       "        [ 1, 23, 37, 30, 43, 40,  2,  0,  0,  0],\n",
       "        [ 1, 54, 44, 50,  2,  0,  0,  0,  0,  0],\n",
       "        [ 1, 48, 44,  2,  0,  0,  0,  0,  0,  0],\n",
       "        [ 1, 42, 50, 32, 37, 66,  2,  0,  0,  0]])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_sequence.reshape(96, 21)[32:37, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we interpret somehow char_sequence without reshaping? Simple example below shows that our 21-dim vectors go in 96-dim vector one-by-one.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(18).reshape(9, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1],\n",
       "       [ 2,  3],\n",
       "       [ 4,  5],\n",
       "       [ 6,  7],\n",
       "       [ 8,  9],\n",
       "       [10, 11],\n",
       "       [12, 13],\n",
       "       [14, 15],\n",
       "       [16, 17]])"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8],\n",
       "       [ 9, 10, 11, 12, 13, 14, 15, 16, 17]])"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(2, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 90, 48, 91,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  1, 12,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  1, 37, 30, 51, 34,  2,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 31, 34, 34, 43,  2,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 31, 41, 44, 52, 43,\n",
       "         2,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_sequence[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 30, 52, 30, 54,  2,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 31, 54,  2,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 49, 37,\n",
       "        38, 48,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         1, 32, 44, 43, 35, 34, 47, 34, 43, 32, 34, 66,  2,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  1, 30, 43])"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_sequence[1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to crop it for `input` and `target`. So we remove actually a few words from the first review. Not starting symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = char_sequence[1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 96])"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 30, 52, 30, 54,  2,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 31, 54,  2,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1, 49, 37,\n",
       "        38, 48,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         1, 32, 44, 43, 35, 34, 47, 34, 43, 32, 34, 66,  2,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  1, 30, 43])"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we first reshape back and then crop? I guess result would be better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "target2 = char_sequence.reshape(96, 21)[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([96, 20])"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[90, 48, 91,  2,  0,  0,  0,  0,  0,  0],\n",
       "        [12,  2,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [37, 30, 51, 34,  2,  0,  0,  0,  0,  0],\n",
       "        [31, 34, 34, 43,  2,  0,  0,  0,  0,  0],\n",
       "        [31, 41, 44, 52, 43,  2,  0,  0,  0,  0]])"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target2[:5, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we actually cropped `start` symbol. It's not clear at all how to crop `end` symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Running Sanity Check for Question 2d: CharDecoder.decode_greedy()\n",
      "--------------------------------------------------------------------------------\n",
      "Sanity Check Passed for Question 2d: CharDecoder.decode_greedy()!\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!python3 sanity_check.py 2d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
