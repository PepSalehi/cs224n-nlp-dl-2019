{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are theoretical questions (a) - (d) of the assignment 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Assignment `4` we used `256-dimensional` word embeddings (`eword = 256`), while in this assignment, it turns out that a character embedding size of `50` suffices (`echar = 50`). In 1-2 sentences, explain one reason why the embedding size used for character-level embeddings is typically lower than that used for word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From *T.Mikolov et. all 2013* (p. 5): \n",
    "\n",
    "- \"Finally, we found that when we train **high dimensional** word vectors on a large amount of data, the resulting vectors can be used to answer very subtle semantic relationships between words, such as a city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin\".\n",
    "\n",
    "- \"It can be seen that after some point, adding more dimensions or adding more training data provides diminishing improvements. So, we have to increase both vector dimensionality and the amount of the training data together. While this observation might seem trivial, it must be noted that it is currently popular to train word vectors on relatively large amounts of data, but with insufficient size (such as 50 - 100).\"\n",
    "\n",
    "- \"We believe that word vectors trained on even larger data sets with larger dimensionality will perform significantly better, and will enable the development of new innovative applications.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we may conclude that different relations between words and different possible word context require much higher dimensions for word embedding than for character-level embedding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down the total number of parameters in the character-based embedding model (`Figure 2`), then do the same for the word-based lookup embedding model (`Figure 1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of word embedding the shape of `Embedding` layer is `(vocab_size, embed_size)` or $(V_{word}, e_{word})$. So in our case we have `50,000 * 256` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12800000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50000 * 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of character embedding we have:\n",
    "\n",
    "- shape of `Embedding` layer is $(V_{char}, e_{char})$;\n",
    "- number of params in `Conv` layer $(f, e_{char}, k)$;\n",
    "- we also have `highway` layer with 2 gates with shape $(e_{word}, e_{word})$;\n",
    "\n",
    "So in this case we have: `96 * 50 + 1 * 50 * 5 + 256^2 * 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5310"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "96 * 50 + 50 * 5 + 256^2 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2410.546139359699"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50000 * 256 / 5310"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In step `3` of the character-based embedding model, instead of using a `1D` convnet, we could have used a `RNN` instead (e.g. feed the sequence of characters into a bidirectional `LSTM` and combine the hidden states using max-pooling). Explain one advantage of using a convolutional architecture rather than a recurrent architecture for this purpose, making it clear how the two contrast. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From *Kim et all, 2015*:\n",
    "\n",
    "- \"While NLMs have been shown to outperform count-based n-gram language models (Mikolov et al. 2011), they are blind to subword information (e.g. morphemes).\"\n",
    "- \"On English, we achieve results on par with the existing state-of-the-art on the Penn Treebank (PTB), despite hav- ing approximately 60% fewer parameters, and\"\n",
    "- \"On morphologically rich languages (Arabic, Czech, French, German, Spanish, and Russian), our model outperforms various baselines (Kneser-Ney, word- level/morpheme-level LSTM), again with fewer parame- ters\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So character-level models give us better results when a language is \"morphologically rich\", in other words when there are a lot of information depends on slight variation in a word. From the article: \"For example, they do not know, a priori, that *eventful, eventfully, uneventful, and uneventfully* should have structurally related embeddings in the vector space.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In lectures we learned about both max-pooling and average-pooling. For each pooling method, please explain one advantage in comparison to the other pooling method. For each advantage, make it clear how the two contrast, and write to a similar level of detail as in the example given in the previous question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's an article that compares different hyperparameter strategies including pooling strategies for the task of sentence classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From *Zhang et all, 2016*:\n",
    "\n",
    "- \"Our analysis of pooling strategies shows that 1- max pooling consistently performs better than al- ternative strategies for the task of sentence clas- sification\". \n",
    "- \"This may be because the location of predictive contexts does not matter, and certain n-grams in the sentence can be more predictive on their own than the entire sentence considered jointly.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we may conclude that 1-max pooling is better (that is consistent with computer vision as mentioned in notes for cs231). This is an empirical observation. We may just try to suggest some reasons why it can be true. And one such reason is mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that concludes the theoretical part of the assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
