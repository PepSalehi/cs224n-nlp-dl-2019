{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from utils.gradcheck import gradcheck_numeric\n",
    "from utils.utils import normalizeRows, softmax\n",
    "\n",
    "import torch\n",
    "\n",
    "from word2vec import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are we going to implement in this assignment? Well we need to implement (both for `naive softmax` and `negative sampling`):\n",
    "\n",
    "- loss function;\n",
    "- backward pass;\n",
    "- SGD;\n",
    "\n",
    "Let's start with `naive softmax`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have to implement `naiveSoftmaxLossAndGradient(centerWordVec, outsideWordIdx, outsideVectors, dataset)`. The main problem here is to understand what all of these arguments mean. Fortunately we have test examples in 2019 version of the course (albeit in not very friendly format) and so we can debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup for training (we don't need dataset for naive softmax)\n",
    "dataset = type('dummy', (), {})()\n",
    "\n",
    "def dummySampleTokenIdx():\n",
    "    return random.randint(0, 4)\n",
    "\n",
    "def getRandomContext(C):\n",
    "    tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    return tokens[random.randint(0, 4)], \\\n",
    "           [tokens[random.randint(0, 4)] for i in range(2 * C)]\n",
    "\n",
    "dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "dataset.getRandomContext = getRandomContext\n",
    "\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "dummy_vectors = normalizeRows(np.random.randn(10, 3))\n",
    "dummy_tokens = dict([(\"a\", 0), (\"b\", 1), (\"c\", 2), (\"d\", 3), (\"e\", 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.96735714, -0.02182641,  0.25247529],\n",
       "       [ 0.73663029, -0.48088687, -0.47552459],\n",
       "       [-0.27323645,  0.12538062,  0.95374082],\n",
       "       [-0.56713774, -0.27178229, -0.77748902],\n",
       "       [-0.59609459,  0.7795666 ,  0.19221644],\n",
       "       [-0.6831809 , -0.04200519,  0.72904007],\n",
       "       [ 0.18289107,  0.76098587, -0.62245591],\n",
       "       [-0.61517874,  0.5147624 , -0.59713884],\n",
       "       [-0.33867074, -0.80966534, -0.47931635],\n",
       "       [-0.52629529, -0.78190408,  0.33412466]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, gradCenterVecs, gradOutsideVectors = \\\n",
    "    skipgram(currentCenterWord=\"c\", \n",
    "             windowSize=3, \n",
    "             outsideWords=[\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"],\n",
    "             word2Ind=dummy_tokens, \n",
    "             centerWordVectors=dummy_vectors[:5, :], \n",
    "             outsideVectors=dummy_vectors[5:, :], \n",
    "             dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.16610900153398"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradCenterVecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [-1.26947339, -1.36873189,  2.45158957],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradCenterVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.41045956,  0.18834851,  1.43272264],\n",
       "       [ 0.38202831, -0.17530219, -1.33348241],\n",
       "       [ 0.07009355, -0.03216399, -0.24466386],\n",
       "       [ 0.09472154, -0.04346509, -0.33062865],\n",
       "       [-0.13638384,  0.06258276,  0.47605228]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradOutsideVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From comments:\n",
    "\n",
    "- `centerWordVec` - numpy ndarray, center word's embedding (`v_c` in the pdf handout)\n",
    "- `outsideWordIdx` - integer, the index of the outside word (`o` of `u_o` in the pdf handout)\n",
    "- `outsideVectors` - outside vectors (rows of matrix) for all words in vocab (`U` in the pdf handout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.27323645,  0.12538062,  0.95374082])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# currentCenterWord = 'c', index of it in dummy_tokens is 2\n",
    "# and we make a lookup in the first part of dummy_vectors\n",
    "centerWordVec = dummy_vectors[2, :]\n",
    "centerWordVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if outsideWord = 'a' then we look up it's index in dummy_tokens\n",
    "outsideWordIdx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.6831809 , -0.04200519,  0.72904007],\n",
       "       [ 0.18289107,  0.76098587, -0.62245591],\n",
       "       [-0.61517874,  0.5147624 , -0.59713884],\n",
       "       [-0.33867074, -0.80966534, -0.47931635],\n",
       "       [-0.52629529, -0.78190408,  0.33412466]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outsideVectors is the second part of dummy_vectors\n",
    "dummy_vectors[5:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first compute probability using formula `(1)` from pdf and then loss using formula `(2)`. But how can we get those probs in code? It seems we may multiply `outsideVectors` and `centerWordVec` to get vector of shape `(5,)`, then use `softmax`. Now we may just take necessary component of it depending of index `o`. As we can see we get correct loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentCenterWord=\"c\"\n",
    "centerWordVec = dummy_vectors[2, :]\n",
    "outsideWords=[\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"]\n",
    "outsideVectors=dummy_vectors[5:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 3), (3,))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outsideVectors.shape, centerWordVec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product = centerWordVec.dot(outsideVectors.T)\n",
    "dot_product.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.6831809 , -0.04200519,  0.72904007])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outsideVectors[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.876718551316616"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outsideVectors[0, :].dot(centerWordVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.876718551316616"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_product_softmax = softmax(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.41703564, 0.10030664, 0.12391154, 0.10888915, 0.24985703])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot_product_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_man = 0\n",
    "probs = softmax(centerWordVec.dot(outsideVectors.T))\n",
    "for outsideWord in outsideWords:\n",
    "    outsideWordIdx = dummy_tokens[outsideWord]\n",
    "    loss_man += -np.log(probs[outsideWordIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.16610900153398"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_man"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient for center vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### theory - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we need to compute gradient of `(3)` with respect to $v_c$. First let's write down loss function and probabilities. The last equation here is an exersice 1 (a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J=CE(y, \\hat{y})=-\\sum_{w}log(\\hat{y}_w)=-log(\\hat{y}_o)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y}_o=p(o|c)=\\frac{\\exp(u^T_o v_c)}{\\sum_{w}\\exp(u^T_w v_c)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a partial derivative with respect to a component of $v_c$:  $\\partial J /\\partial v_{cj}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J}{\\partial v_{cj}} = \\frac{\\partial (-log(\\hat{y}_o))}{\\partial v_{cj}} = \\frac{\\partial (-u^T_o v_c + \\log(\\sum_{w}\\exp(u^T_w v_c)))}{\\partial v_{cj}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First term is just $-u_{oj}$. To compute the second term we need to understand that:\n",
    "\n",
    "$$\\frac{\\exp(u^T_{w^\\prime} v_c)}{\\sum_{w}\\exp(u^T_w v_c))}=\\hat{y}_{w^\\prime}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we get (with or without index `j`):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J}{\\partial v_{cj}} = -u_{oj} + \\sum_{w} \\hat{y}_w u_{wj}$$\n",
    "$$\\frac{\\partial J}{\\partial v_{c}} = -u_{o} + \\sum_{w} \\hat{y}_w u_{w}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two questions left: \n",
    "\n",
    "- can we get this result directly without using index `j`? and \n",
    "- also can we get vectorized representation of this result;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the following formula: $\\partial (u^T v) / \\partial v = u$ that can be checked directly. Now we can get first term directly. And probably the second one as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider $U y$ where *columns* of $U$ consists of our $u_i$ vectors. If $y$ is one-hot-encoded with $y_o = 1$ then $U y = u_o$. So the first term in our formula is just $-Uy$. \n",
    "\n",
    "The second term is just a combination of columns of $U$ and we know that this is the same as $U \\hat{y}$. So we simply get:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial v_{c}} = U (\\hat{y} - y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our derivation. Probably next time we won't be so detailed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### theory - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next question - how should we accumulate those gradients for `center` vector while we're iterating over the window? If we look at the `skip-gram` loss function (for simplicity we skip all indicies of $w$ - see detailed formula in `pdf`) then pretty much obvious that we have to sum up those gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J_{skip-gram}(v_c, w, U) = \\sum_{w} J(v_c, w, U)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing that we should be careful - we compute here gradient of only one vector $v_c$. So we have to write code like this: `gradCenterVecs[currentCenterWordIdx] += gradCenterVec`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to get these gradients in our example. `probs` is the same as $\\hat y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centerWordVectors = dummy_vectors[:5, :]\n",
    "gradCenterVecs = np.zeros(centerWordVectors.shape)\n",
    "gradCenterVecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "currentCenterWord=\"c\"\n",
    "word2Ind = dummy_tokens\n",
    "currentCenterWordIdx = word2Ind[currentCenterWord]\n",
    "currentCenterWordIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for outsideWord in outsideWords:\n",
    "    # compute grad for one outside word\n",
    "    outsideWordIdx = word2Ind[outsideWord]\n",
    "    y = np.zeros(len(word2Ind))  \n",
    "    y[outsideWordIdx] = 1\n",
    "    gradCenterVec = outsideVectors.T.dot(probs - y)\n",
    "    \n",
    "    # add it to accumulated amount\n",
    "    gradCenterVecs[currentCenterWordIdx] += gradCenterVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [-1.26947339, -1.36873189,  2.45158957],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradCenterVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradient for outside vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not going to reproduce here detailed derivations and just state the formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial J}{\\partial U} = v_c (\\hat{y} - y)^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to reproduce computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradOutsideVectors = np.zeros(outsideVectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for outsideWord in outsideWords:\n",
    "    # compute grad for one outside word\n",
    "    outsideWordIdx = word2Ind[outsideWord]\n",
    "    y = np.zeros(len(word2Ind))  \n",
    "    y[outsideWordIdx] = 1\n",
    "    gradOutsideVecs = np.outer((probs - y), centerWordVec)\n",
    "    \n",
    "    # add it to accumulated amount\n",
    "    gradOutsideVectors += gradOutsideVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.41045956,  0.18834851,  1.43272264],\n",
       "       [ 0.38202831, -0.17530219, -1.33348241],\n",
       "       [ 0.07009355, -0.03216399, -0.24466386],\n",
       "       [ 0.09472154, -0.04346509, -0.33062865],\n",
       "       [-0.13638384,  0.06258276,  0.47605228]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradOutsideVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That concludes our computations for `naive softmax case`. Next we have to look at `negative sampling`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
