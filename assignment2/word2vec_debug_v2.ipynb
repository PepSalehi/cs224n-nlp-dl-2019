{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from utils.gradcheck import gradcheck_numeric\n",
    "from utils.utils import normalizeRows, softmax\n",
    "\n",
    "import torch\n",
    "\n",
    "from word2vec import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now it's time for more complicated `negative sampling`. That's the loss function that we're  going to train on our corpus. Code is going to be more complicated - we need to use vectorization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(s):\n",
    "    random.seed(s)\n",
    "    np.random.seed(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup for training\n",
    "dataset = type('dummy', (), {})()\n",
    "\n",
    "def dummySampleTokenIdx():\n",
    "    return random.randint(0, 4)\n",
    "\n",
    "def getRandomContext(C):\n",
    "    tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    return tokens[random.randint(0, 4)], \\\n",
    "           [tokens[random.randint(0, 4)] for i in range(2 * C)]\n",
    "\n",
    "dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "dataset.getRandomContext = getRandomContext\n",
    "\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "dummy_vectors = normalizeRows(np.random.randn(10, 3))\n",
    "dummy_tokens = dict([(\"a\", 0), (\"b\", 1), (\"c\", 2), (\"d\", 3), (\"e\", 4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.96735714, -0.02182641,  0.25247529],\n",
       "       [ 0.73663029, -0.48088687, -0.47552459],\n",
       "       [-0.27323645,  0.12538062,  0.95374082],\n",
       "       [-0.56713774, -0.27178229, -0.77748902],\n",
       "       [-0.59609459,  0.7795666 ,  0.19221644],\n",
       "       [-0.6831809 , -0.04200519,  0.72904007],\n",
       "       [ 0.18289107,  0.76098587, -0.62245591],\n",
       "       [-0.61517874,  0.5147624 , -0.59713884],\n",
       "       [-0.33867074, -0.80966534, -0.47931635],\n",
       "       [-0.52629529, -0.78190408,  0.33412466]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram_v2(word2vecLossAndGradient=negSamplingLossAndGradient):\n",
    "    set_seed(42)\n",
    "    return skipgram(currentCenterWord=\"c\", \n",
    "                    windowSize=1, \n",
    "                    outsideWords=[\"a\", \"b\"],\n",
    "                    word2Ind=dummy_tokens, \n",
    "                    centerWordVectors=dummy_vectors[:5, :], \n",
    "                    outsideVectors=dummy_vectors[5:, :], \n",
    "                    dataset=dataset,\n",
    "                    word2vecLossAndGradient=word2vecLossAndGradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, gradCenterVecs, gradOutsideVectors = skipgram_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.35112555003494"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [-3.75386588, -3.18521139,  0.21341777],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradCenterVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.30559455,  0.14022886,  1.06668785],\n",
       "       [-0.32724523,  0.15016375,  1.14226025],\n",
       "       [-0.22764219,  0.10445868,  0.79459256],\n",
       "       [-0.42136814,  0.19335415,  1.47079939],\n",
       "       [-1.12868414,  0.51792184,  3.93970919]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradOutsideVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getNegativeSamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "negSampleWordIndices = getNegativeSamples(outsideWordIdx=0, \n",
    "                                          dataset=dataset, \n",
    "                                          K=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 1, 1, 4, 4, 3, 1, 1, 4]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negSampleWordIndices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How exactly do we generate those indicies? Well we have 3 arguments: `outsideWordIdx, dataset, K`. We iterate over `range(K)` and call `dataset.sampleTokenIdx()` that simply generate random number in `[0, 1, 2, 3, 4]`.\n",
    "\n",
    "In actual training we use class `StanfordSentiment` that also has method `sampleTokenIdx`. It's analysis is clearly outside of scope of this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.sampleTokenIdx()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradCenterVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with theory but keep it simple here. To find gradient of the first term we need to use chain rule: `grad` of `log` multiply by `grad` of `sigmoid` and finally by `grad` of vector product. It's easy to see the result: $-\\sigma (1-\\sigma) / \\sigma \\cdot \\partial(u^T_o v_c) / \\partial v_c$. We may use the same trick for the second term as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J_{neg-sample}(v_c, o, U) = -\\log(\\sigma(u^T_o v_c)) - \\sum_{k}\\log(\\sigma(-u^T_k v_c))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial J_{neg-sample}}{\\partial v_{c}} = (\\sigma(u^T_o v_c)-1)u_o - \\sum_{k}(\\sigma(-u^T_k v_c)-1)u_k $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negSamplingLossAndGradient_v2(centerWordVec, outsideWordIdx, \n",
    "                               outsideVectors, dataset, K=10):\n",
    "    \n",
    "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
    "    indices = [outsideWordIdx] + negSampleWordIndices\n",
    "    \n",
    "    uo = outsideVectors[outsideWordIdx]\n",
    "    vc = centerWordVec\n",
    "    Uneg = outsideVectors[negSampleWordIndices, :]\n",
    "    \n",
    "    loss = -np.log(sigmoid(uo.dot(vc))) \\\n",
    "           - np.sum(np.log(sigmoid(-Uneg.dot(vc))))\n",
    "    \n",
    "    gradCenterVecs = (sigmoid(uo.dot(vc)) - 1) * uo \\\n",
    "                   - Uneg.T.dot(sigmoid(-Uneg.dot(vc)) - 1)\n",
    "    \n",
    "    return loss, gradCenterVecs, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, gradCenterVecs, gradOutsideVectors = \\\n",
    "    skipgram_v2(word2vecLossAndGradient=negSamplingLossAndGradient_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.35112555003494"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [-3.75386588, -3.18521139,  0.21341777],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradCenterVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vectorization of sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look in details what `Uneg.dot(vc)` is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "negSampleWordIndices = getNegativeSamples(outsideWordIdx=0, \n",
    "                                          dataset=dataset, \n",
    "                                          K=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2, 1, 1, 1, 4, 4, 3, 1, 1, 4], 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negSampleWordIndices, len(negSampleWordIndices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "outsideVectors = dummy_vectors[5:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectors u are rows of this matrix\n",
    "outsideVectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows of Uneg - vectors u with indicies in negSampleWordIndices\n",
    "Uneg = outsideVectors[negSampleWordIndices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Uneg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.61517874,  0.5147624 , -0.59713884])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this should be equal to outsideVectors[2]\n",
    "Uneg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.61517874,  0.5147624 , -0.59713884])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and indeed it's equal\n",
    "outsideVectors[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "centerWordVectors=dummy_vectors[:5, :]\n",
    "centerWordVec = centerWordVectors[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.27323645,  0.12538062,  0.95374082])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centerWordVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = centerWordVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's compare dot products (that is used in our formulas) and matrix product that we use in code. As we may see result is the same. Why is that? Well that's just another form of matrix multiplication:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Uv = \\begin{bmatrix} u^T_1\\\\u^T_2 \\\\ ... \\end{bmatrix} v = \\begin{bmatrix} u^T_1 v\\\\u^T_2 v \\\\ ... \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.33688520543359074,\n",
       " -0.5482212337396852,\n",
       " -0.5482212337396852,\n",
       " -0.5482212337396852,\n",
       " 0.3644357565240586,\n",
       " 0.3644357565240586,\n",
       " -0.46612272591787834,\n",
       " -0.5482212337396852,\n",
       " -0.5482212337396852,\n",
       " 0.3644357565240586]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[Uneg[i, :].dot(vc) for i in range(Uneg.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.33688521, -0.54822123, -0.54822123, -0.54822123,  0.36443576,\n",
       "        0.36443576, -0.46612273, -0.54822123, -0.54822123,  0.36443576])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Uneg.dot(vc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gradCenterVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial J_{neg-sample}}{\\partial v_{c}} = (\\sigma(u^T_o v_c)-1)u_o - \\sum_{k}(\\sigma(-u^T_k v_c)-1)u_k $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have even more involved vectorization: `Uneg.T.dot(sigmoid(-Uneg.dot(vc)) - 1)`. Why is it true? Well we already saw that `Uneg.dot(vc)` is vector with components $u^T_k v_c$. \n",
    "\n",
    "We now need linear combination of *rows* of $U$ by some vector. We know that to get that we need to multiply transposed $U$ by this vector. So again we use just modification of matrix multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gradOutsideVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally lets try to analyze `gradOutsideVecs`. They can't be vectorized completely due to repetitions in `negSampleWordIndices`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial J_{neg-sample}}{\\partial u_{o}} = (\\sigma(u^T_o v_c)-1) v_c $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial J_{neg-sample}}{\\partial u_k} = -(\\sigma(-u^T_k v_c)-1) v_c \\ \\ \\ k=1, 2,...,K $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below should be clear for now. To compute `gradOutsideVecs[o, :]` we don't need any vectorization at all. `Uneg.dot(vc)` is again the matrix of $u^T_k v_c$. And now we have a little trick: if $u_i = u_j$ we have to add those gradients to `gradOutsideVecs[k, :]` where `k == i == j`. To do this using vectorization we need one more trick: we iterate over `zip(range(K), negSampleWordIndices)` (not over only `negSampleWordIndices`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negSamplingLossAndGradient_v3(centerWordVec, outsideWordIdx, \n",
    "                               outsideVectors, dataset, K=10):\n",
    "    \n",
    "    negSampleWordIndices = getNegativeSamples(outsideWordIdx, dataset, K)\n",
    "    indices = [outsideWordIdx] + negSampleWordIndices\n",
    "    \n",
    "    o = outsideWordIdx\n",
    "    uo = outsideVectors[outsideWordIdx]\n",
    "    vc = centerWordVec\n",
    "    Uneg = outsideVectors[negSampleWordIndices, :]\n",
    "    \n",
    "    print(negSampleWordIndices)\n",
    "    print(Uneg)\n",
    "    \n",
    "    gradOutsideVecs = np.zeros_like(outsideVectors)\n",
    "    gradOutsideVecs[o, :] = (sigmoid(uo.dot(vc)) - 1) * vc\n",
    "    s = -(sigmoid(-Uneg.dot(vc)) - 1)\n",
    "    for i, k in zip(range(K), negSampleWordIndices):\n",
    "        gradOutsideVecs[k, :] += s[i] * vc\n",
    "    \n",
    "    \n",
    "    return loss, 0, gradOutsideVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 1, 1, 4, 4, 3, 1, 1, 4]\n",
      "[[-0.61517874  0.5147624  -0.59713884]\n",
      " [ 0.18289107  0.76098587 -0.62245591]\n",
      " [ 0.18289107  0.76098587 -0.62245591]\n",
      " [ 0.18289107  0.76098587 -0.62245591]\n",
      " [-0.52629529 -0.78190408  0.33412466]\n",
      " [-0.52629529 -0.78190408  0.33412466]\n",
      " [-0.33867074 -0.80966534 -0.47931635]\n",
      " [ 0.18289107  0.76098587 -0.62245591]\n",
      " [ 0.18289107  0.76098587 -0.62245591]\n",
      " [-0.52629529 -0.78190408  0.33412466]]\n",
      "[4, 0, 4, 4, 3, 3, 4, 2, 0, 3]\n",
      "[[-0.52629529 -0.78190408  0.33412466]\n",
      " [-0.6831809  -0.04200519  0.72904007]\n",
      " [-0.52629529 -0.78190408  0.33412466]\n",
      " [-0.52629529 -0.78190408  0.33412466]\n",
      " [-0.33867074 -0.80966534 -0.47931635]\n",
      " [-0.33867074 -0.80966534 -0.47931635]\n",
      " [-0.52629529 -0.78190408  0.33412466]\n",
      " [-0.61517874  0.5147624  -0.59713884]\n",
      " [-0.6831809  -0.04200519  0.72904007]\n",
      " [-0.33867074 -0.80966534 -0.47931635]]\n"
     ]
    }
   ],
   "source": [
    "_, _, gradOutsideVectors = \\\n",
    "    skipgram_v2(word2vecLossAndGradient=negSamplingLossAndGradient_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.30559455,  0.14022886,  1.06668785],\n",
       "       [-0.32724523,  0.15016375,  1.14226025],\n",
       "       [-0.22764219,  0.10445868,  0.79459256],\n",
       "       [-0.42136814,  0.19335415,  1.47079939],\n",
       "       [-1.12868414,  0.51792184,  3.93970919]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradOutsideVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
