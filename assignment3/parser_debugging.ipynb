{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from parser_transitions import *\n",
    "from run import *\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `minibatch_parse`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `DummyModel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First question is what `DummyModel` doing? Actually there's a good description of what it's doing in comments. Let's just check that it's correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [[\"right\", \"arcs\", \"only\"],\n",
    "             [\"right\", \"arcs\", \"only\", \"again\"],\n",
    "             [\"left\", \"arcs\", \"only\"],\n",
    "             [\"left\", \"arcs\", \"only\", \"again\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = PartialParse(sentence=sentences[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = DummyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ROOT'] ['right', 'arcs', 'only'] ['S']\n",
      "['ROOT', 'right'] ['arcs', 'only'] ['S']\n",
      "['ROOT', 'right', 'arcs'] ['only'] ['S']\n",
      "['ROOT', 'right', 'arcs', 'only'] [] ['RA']\n",
      "['ROOT', 'right', 'arcs'] [] ['RA']\n",
      "['ROOT', 'right'] [] ['RA']\n",
      "['ROOT'] [] []\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    t = dm.predict([pp])\n",
    "    print(pp.stack, pp.buffer, t)\n",
    "    pp.parse_step(t[0])\n",
    "    if pp.is_empty():\n",
    "        print(pp.stack, pp.buffer, [])\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `test_minibatch_parse()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "deps = minibatch_parse(sentences=sentences,\n",
    "                       model=DummyModel(),\n",
    "                       batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('arcs', 'only'), ('right', 'arcs'), ('ROOT', 'right')],\n",
       " [('only', 'again'), ('arcs', 'only'), ('right', 'arcs'), ('ROOT', 'right')],\n",
       " [('only', 'arcs'), ('only', 'left'), ('only', 'ROOT')],\n",
       " [('again', 'only'), ('again', 'arcs'), ('again', 'left'), ('again', 'ROOT')]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this correct parsing? Let's look at `[\"right\", \"arcs\", \"only\"]`. We should have only right arcs here: `arcs -> only`, `right -> arcs` and `root -> right`. And that's exactly what we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_sentences = [['I', 'parsed', 'this', 'sentence', 'correctly']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a model that can parse this sentence from `pdf`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ParserModel`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Embedding layer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First question - what is the shape of our `Embedding` layer and how do we set pre-computed weights? As usual shape is `(vocab_size, embed_size)` - see below. \n",
    "\n",
    "It seems that the proper method to use pre-computed embeddings is to use `from_pretrained()`. It looks like the approach from provided code is qustionable. In particular we don't freeze training as we can see below. \n",
    "\n",
    "From documantation:\n",
    "- `freeze (boolean, optional) â€“ If True, the tensor does not get updated in the learning process. Equivalent to embedding.weight.requires_grad = False. Default: True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "took 2.84 seconds\n",
      "Building parser...\n",
      "took 0.04 seconds\n",
      "Loading pretrained embeddings...\n",
      "took 3.00 seconds\n",
      "Vectorizing data...\n",
      "took 0.08 seconds\n",
      "Preprocessing training data...\n",
      "took 1.50 seconds\n"
     ]
    }
   ],
   "source": [
    "parser, embeddings, train_data, dev_data, test_data = load_and_preprocess_data(reduced=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5157, 50)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = nn.Embedding(*embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5157, 50])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.weight.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer.weight = nn.Parameter(torch.tensor(embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.weight.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next question - how do feed data in `Embedding` layer and then to `Linear` layer. It seems that the input is a list of integers where an integer represents a token in vocabulary as usual. Output of `Embedding` layer is `(batch_size, seq_len, embed_size)`. In our case `seq_len` is just a number of features.\n",
    "\n",
    "To feed data into `Linear` layer we need to flatten them into `2D`. And we can do it like `(32, 8 * 50)` or `(32 * 8, 50)`. What's better? It seems I used both of these methods but in this assignment they recommend the first option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.LongTensor(np.arange(256).reshape(32, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_embed = embedding_layer(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8, 50])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xavier init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we're supposed to use `Xavier` initialization in this assignment. We have the following questions:\n",
    "\n",
    "- is it the same as `Glorot` init?\n",
    "- what is the difference with the default init for a `Linear` layer?\n",
    "- what is better to use with `relu` activation function?\n",
    "- what is `He` init?\n",
    "- what is code in `pytorch`?\n",
    "- finally, what is math behind this?\n",
    "\n",
    "It looks like the default init in `pytorch` is not `He` init. So in case we use `relu activation` we have to change this init.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### is it the same as Glorot init?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all this init is based on *Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010)* and the leading author is Xavier Glorot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what is the difference with the default init for a `Linear` layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `pytorch` docs we may see that the values are initialized from $U(-\\sqrt{k}, \\sqrt{k})$ where $k = 1 / f_{in}$. This is precisely heuristic that is used for comparison in the article (`eq. 1`). \n",
    "\n",
    "`Xavier` init is from $U(-a, a)$ where $a=\\sqrt{\\frac{6}{f_{in}+f_{out}}}$. This is specified in `pytorch` docs and in the paper (`eq. 16`). In `pytorch` there's additional parameter: `gain`. So $a=gain \\cdot \\sqrt{\\frac{6}{f_{in}+f_{out}}}$. For `relu` $gain=\\sqrt{2}$. It's not clear why they use this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is the case with `uniform` init. In case of `normal` init we have $\\sigma=gain \\cdot \\sqrt{\\frac{2}{f_{in}+f_{out}}}$ and we use $\\mathcal{N}(0, \\sigma^2)$. And again $gain=\\sqrt{2}$ so basically we use `He` init.\n",
    "\n",
    "In Andrew Ng's lectures we may find a variant of this init: $\\sigma=\\sqrt{\\frac{1}{f_{in}}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what is better to use with relu activation function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I understand it's better to use `He` init (for example, see Andrew Ng's lectures `C2W1L11`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `He` normal init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "paper: *Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification. He et. all (2015)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $\\sigma=\\sqrt{\\frac{2}{f_{in}}}$ and we use $\\mathcal{N}(0, \\sigma^2)$ (`eq. 10` in the article)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what is code in pytorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Xavier` `uniform`: `nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`He` `uniform`: `nn.init.kaiming_uniform_(w)`;\n",
    "`He` `normal`: `nn.init.kaiming_normal_(w)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the [post](https://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization) that is mentioned in assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So `Xavier` init draws from $\\mathcal{N}(0, \\sigma^2)$ where $\\sigma^2=\\frac{2}{f_{in} + f_{out}}$. This is the formula from their original paper but often used another formula: $\\sigma^2=\\frac{1}{f_{in}}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all suppose $X \\perp Y$ and also $\\mathbb{E}(X) = \\mathbb{E}(Y) = 0$. Then it's easy to show that $\\mathbb{V}(XY) = \\mathbb{V}(X)\\mathbb{V}(Y)$. \n",
    "\n",
    "Let's use the formula: $\\mathbb{V}(XY) = \\mathbb{E}(X^2Y^2)-(\\mathbb{E}(XY))^2$. First term is $\\mathbb{E}(X^2)\\mathbb{E}(Y^2) = \\mathbb{V}(X)\\mathbb{V}(Y)$ since $X^2 \\perp Y^2$ and we can factorize expectation of product for independent random variables. And the second term is 0 for similar reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider a single neuron: $Y = W_1X_1 + ... + W_nX_n$ where $W_i \\perp X_i$ and so on.\n",
    "\n",
    "In this case we have: $\\mathbb{V}(Y) = n\\mathbb{V}(W)\\mathbb{V}(X)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the **key assumption**: we need $\\mathbb{V}(X)=\\mathbb{V}(Y)$. Why is that? Probably we actually need $\\mathbb{V}(X) \\sim \\mathbb{V}(Y)$ - in other words an input signal must not explode or shrink. \n",
    "\n",
    "So we have: $\\mathbb{V}(W) = \\frac{1}{n} = \\frac{1}{n_{in}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to get the original formula? Well we need consider the backprop and take an average ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
